{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77447b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in d:\\python env\\venv\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in d:\\python env\\venv\\lib\\site-packages (from nltk) (1.5.0)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ------------------ ------------------- 20.5/41.5 kB 640.0 kB/s eta 0:00:01\n",
      "     ---------------------------------------- 41.5/41.5 kB 1.0 MB/s eta 0:00:00\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in d:\\python env\\venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.2/1.5 MB 12.3 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.3/1.5 MB 9.6 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.3/1.5 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.6/1.5 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.6/1.5 MB 4.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 4.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.5 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.5 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.5 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.5 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.5 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.5 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.5 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.5 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.5 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
      "   ---------------------------------------- 0.0/273.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/273.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/273.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/273.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/273.6 kB ? eta -:--:--\n",
      "   ----- --------------------------------- 41.0/273.6 kB 991.0 kB/s eta 0:00:01\n",
      "   ---------- ---------------------------- 71.7/273.6 kB 653.6 kB/s eta 0:00:01\n",
      "   -------------- ----------------------- 102.4/273.6 kB 837.8 kB/s eta 0:00:01\n",
      "   -------------- ----------------------- 102.4/273.6 kB 837.8 kB/s eta 0:00:01\n",
      "   -------------- ----------------------- 102.4/273.6 kB 837.8 kB/s eta 0:00:01\n",
      "   -------------- ----------------------- 102.4/273.6 kB 837.8 kB/s eta 0:00:01\n",
      "   -------------- ----------------------- 102.4/273.6 kB 837.8 kB/s eta 0:00:01\n",
      "   -------------- ----------------------- 102.4/273.6 kB 837.8 kB/s eta 0:00:01\n",
      "   -------------- ----------------------- 102.4/273.6 kB 837.8 kB/s eta 0:00:01\n",
      "   --------------------- ---------------- 153.6/273.6 kB 327.3 kB/s eta 0:00:01\n",
      "   ------------------------ ------------- 174.1/273.6 kB 337.8 kB/s eta 0:00:01\n",
      "   ------------------------- ------------ 184.3/273.6 kB 327.8 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 204.8/273.6 kB 336.6 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 235.5/273.6 kB 360.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- 273.6/273.6 kB 391.9 kB/s eta 0:00:00\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, nltk\n",
      "Successfully installed nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bcad97",
   "metadata": {},
   "source": [
    "# Tokenization (Breaking text into words or sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1426ab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9f6a7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Jeevan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5291646c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Sir!', 'Welcome to NLP.', \"Let's start learning.\"]\n",
      "['Hello', 'Sir', '!', 'Welcome', 'to', 'NLP', '.', 'Let', \"'s\", 'start', 'learning', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "\n",
    "text = \"Hello Sir! Welcome to NLP. Let's start learning.\"\n",
    "\n",
    "print(sent_tokenize(text))  # Splits into sentences\n",
    "print(word_tokenize(text))  # Splits into words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e754f97c",
   "metadata": {},
   "source": [
    "## wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0f4bc54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Sir',\n",
       " '!',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'NLP',\n",
       " '.',\n",
       " 'Let',\n",
       " \"'\",\n",
       " 's',\n",
       " 'start',\n",
       " 'learning',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import wordpunct_tokenize\n",
    "\n",
    "wordpunct_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3633736b",
   "metadata": {},
   "source": [
    "Here, 's is also splitted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b767ea",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db411f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jeevan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "93aeaa88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thank', 'much', 'thank', 'academi', 'thank', 'room', 'congratul', 'incred', 'nomine', 'year', 'reven', 'product', 'tireless', 'effort', 'unbeliev', 'cast', 'crew', 'make', 'reven', 'man', 'relationship', 'natur', 'world']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "text = \"\"\"Thank you all so very much. Thank you to the Academy. Thank you to all of you in this room. I have to congratulate the other incredible nominees this year. The Revenant was the product of the tireless efforts of an unbelievable cast and crew. Making The Revenant was about man's relationship to the natural world.\"\"\"\n",
    "\n",
    "words = word_tokenize(text)\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered = [stemmer.stem(word.lower()) for word in words if word.lower() not in stop_words and word.isalpha()]\n",
    "print(filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46f8ffe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "732b3b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"Thank you all so very much. Thank you to the Academy. \n",
    "               Thank you to all of you in this room. I have to congratulate \n",
    "               the other incredible nominees this year. The Revenant was \n",
    "               the product of the tireless efforts of an unbelievable cast\n",
    "               and crew. First off, to my brother in this endeavor, Mr. Tom \n",
    "               Hardy. Tom, your talent on screen can only be surpassed by \n",
    "               your friendship off screen … thank you for creating a t\n",
    "               ranscendent cinematic experience. Thank you to everybody at \n",
    "               Fox and New Regency … my entire team. I have to thank \n",
    "               everyone from the very onset of my career … To my parents; \n",
    "               none of this would be possible without you. And to my \n",
    "               friends, I love you dearly; you know who you are. And lastly,\n",
    "               I just want to say this: Making The Revenant was about\n",
    "               man's relationship to the natural world. A world that we\n",
    "               collectively felt in 2015 as the hottest year in recorded\n",
    "               history. Our production needed to move to the southern\n",
    "               tip of this planet just to be able to find snow. Climate\n",
    "               change is real, it is happening right now. It is the most\n",
    "               urgent threat facing our entire species, and we need to work\n",
    "               collectively together and stop procrastinating. We need to\n",
    "               support leaders around the world who do not speak for the \n",
    "               big polluters, but who speak for all of humanity, for the\n",
    "               indigenous people of the world, for the billions and \n",
    "               billions of underprivileged people out there who would be\n",
    "               most affected by this. For our children’s children, and \n",
    "               for those people out there whose voices have been drowned\n",
    "               out by the politics of greed. I thank you all for this \n",
    "               amazing award tonight. Let us not take this planet for \n",
    "               granted. I do not take tonight for granted. Thank you so very much.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23ba5ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thank',\n",
       " 'much',\n",
       " 'Thank',\n",
       " 'Academy',\n",
       " 'Thank',\n",
       " 'room',\n",
       " 'congratulate',\n",
       " 'incredible',\n",
       " 'nominees',\n",
       " 'year',\n",
       " 'Revenant',\n",
       " 'product',\n",
       " 'tireless',\n",
       " 'efforts',\n",
       " 'unbelievable',\n",
       " 'cast',\n",
       " 'crew',\n",
       " 'First',\n",
       " 'brother',\n",
       " 'endeavor',\n",
       " 'Tom',\n",
       " 'Hardy',\n",
       " 'Tom',\n",
       " 'talent',\n",
       " 'screen',\n",
       " 'surpassed',\n",
       " 'friendship',\n",
       " 'screen',\n",
       " 'thank',\n",
       " 'creating',\n",
       " 'ranscendent',\n",
       " 'cinematic',\n",
       " 'experience',\n",
       " 'Thank',\n",
       " 'everybody',\n",
       " 'Fox',\n",
       " 'New',\n",
       " 'Regency',\n",
       " 'entire',\n",
       " 'team',\n",
       " 'thank',\n",
       " 'everyone',\n",
       " 'onset',\n",
       " 'career',\n",
       " 'parents',\n",
       " 'none',\n",
       " 'would',\n",
       " 'possible',\n",
       " 'without',\n",
       " 'friends',\n",
       " 'love',\n",
       " 'dearly',\n",
       " 'know',\n",
       " 'lastly',\n",
       " 'want',\n",
       " 'say',\n",
       " 'Making',\n",
       " 'Revenant',\n",
       " 'man',\n",
       " 'relationship',\n",
       " 'natural',\n",
       " 'world',\n",
       " 'world',\n",
       " 'collectively',\n",
       " 'felt',\n",
       " 'hottest',\n",
       " 'year',\n",
       " 'recorded',\n",
       " 'history',\n",
       " 'production',\n",
       " 'needed',\n",
       " 'move',\n",
       " 'southern',\n",
       " 'tip',\n",
       " 'planet',\n",
       " 'able',\n",
       " 'find',\n",
       " 'snow',\n",
       " 'Climate',\n",
       " 'change',\n",
       " 'real',\n",
       " 'happening',\n",
       " 'right',\n",
       " 'urgent',\n",
       " 'threat',\n",
       " 'facing',\n",
       " 'entire',\n",
       " 'species',\n",
       " 'need',\n",
       " 'work',\n",
       " 'collectively',\n",
       " 'together',\n",
       " 'stop',\n",
       " 'procrastinating',\n",
       " 'need',\n",
       " 'support',\n",
       " 'leaders',\n",
       " 'around',\n",
       " 'world',\n",
       " 'speak',\n",
       " 'big',\n",
       " 'polluters',\n",
       " 'speak',\n",
       " 'humanity',\n",
       " 'indigenous',\n",
       " 'people',\n",
       " 'world',\n",
       " 'billions',\n",
       " 'billions',\n",
       " 'underprivileged',\n",
       " 'people',\n",
       " 'would',\n",
       " 'affected',\n",
       " 'children',\n",
       " 'children',\n",
       " 'people',\n",
       " 'whose',\n",
       " 'voices',\n",
       " 'drowned',\n",
       " 'politics',\n",
       " 'greed',\n",
       " 'thank',\n",
       " 'amazing',\n",
       " 'award',\n",
       " 'tonight',\n",
       " 'Let',\n",
       " 'us',\n",
       " 'take',\n",
       " 'planet',\n",
       " 'granted',\n",
       " 'take',\n",
       " 'tonight',\n",
       " 'granted',\n",
       " 'Thank',\n",
       " 'much']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "words= word_tokenize(paragraph)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "[word for word in words if word.lower() not in stop_words and word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec340ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "384f2406",
   "metadata": {},
   "source": [
    "#  Stemming (Reducing words to root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef9d4e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running ➝ run\n",
      "runs ➝ run\n",
      "happiness ➝ happi\n",
      "fairly ➝ fairli\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"running\", \"runs\", \"happiness\", \"fairly\"]\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word} ➝ {stemmer.stem(word)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7837e3e6",
   "metadata": {},
   "source": [
    "# Lemmatization (More accurate than stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dad78a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jeevan\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e28402b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running ➝ running\n",
      "flies ➝ fly\n",
      "better ➝ better\n",
      "feet ➝ foot\n",
      "happiness ➝ happiness\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [\"running\", \"flies\", \"better\", \"feet\",\"happiness\"]\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word} ➝ {lemmatizer.lemmatize(word)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eec9c9",
   "metadata": {},
   "source": [
    "### Note: For accurate results, specify POS tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17f101d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('running', pos='v')  # 'run'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f117c434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happiness'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('happiness', pos='n')  # 'run'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cb80b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4badd0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thank', 'much', 'thank', 'academy', 'thank', 'room', 'congratulate', 'incredible', 'nominee', 'year', 'revenant', 'product', 'tireless', 'effort', 'unbelievable', 'cast', 'crew', 'making', 'revenant', 'man', 'relationship', 'natural', 'world']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "text = \"\"\"Thank you all so very much. Thank you to the Academy. Thank you to all of you in this room. I have to congratulate the other incredible nominees this year. The Revenant was the product of the tireless efforts of an unbelievable cast and crew. Making The Revenant was about man's relationship to the natural world.\"\"\"\n",
    "\n",
    "words = word_tokenize(text)\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered = [lemmatizer.lemmatize(word.lower()) for word in words if word.lower() not in stop_words and word.isalpha()]\n",
    "print(filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22558bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happi\n",
      "happiness\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer  \n",
    "\n",
    "stemmer = PorterStemmer()  \n",
    "print(stemmer.stem(\"happiness\"))  # \"happi\"  \n",
    "\n",
    "lemmatizer = WordNetLemmatizer()  \n",
    "print(lemmatizer.lemmatize(\"happiness\", pos='n'))  # \"happiness\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f486795a",
   "metadata": {},
   "source": [
    "# Part of Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59d28ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Jeevan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "38fa96e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "tokens = word_tokenize(text)\n",
    "tags = pos_tag(tokens)\n",
    "\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c6701d",
   "metadata": {},
   "source": [
    "###\n",
    "- `NN` = Noun\n",
    "- `VB` = Verb\n",
    "- `JJ` = Adjective\n",
    "- `RB` = Adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "880cf9b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Taj Mahal is a beautiful monument.', 'NN')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag([\"Taj Mahal is a beautiful monument.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674fd7d5",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "98ec93c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Jeevan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c53c38e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\Jeevan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c2ed8985",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Jeevan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a910fe4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Barack/NNP)\n",
      "  (PERSON Obama/NNP)\n",
      "  was/VBD\n",
      "  born/VBN\n",
      "  in/IN\n",
      "  (GPE Hawaii/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk, pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"Barack Obama was born in Hawaii.\"\n",
    "tokens = word_tokenize(sentence)\n",
    "tags = pos_tag(tokens)\n",
    "\n",
    "tree = ne_chunk(tags)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "16014502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (PERSON Rahul/NNP) (PERSON Dravid/NNP) is/VBZ the/DT wall/NN ./.)\n"
     ]
    }
   ],
   "source": [
    "sentence2 = \"Rahul Dravid is the wall.\"\n",
    "tokens2 = word_tokenize(sentence2)\n",
    "tags2 = pos_tag(tokens2)\n",
    "\n",
    "tree2 = ne_chunk(tags2)\n",
    "print(tree2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97250200",
   "metadata": {},
   "source": [
    "#  Text Corpus & Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "355b677d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Jeevan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\gutenberg.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e34c433b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n",
      "[Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
      "and happy disposition, seemed to unite some of the best blessings\n",
      "of existence; and had lived nearly twenty-one years in the world\n",
      "with very little to distress or vex her.\n",
      "\n",
      "She was the youngest of the two daughters of a most affectionate,\n",
      "indulgent father; and had, in consequence of her sister's marriage,\n",
      "been mistress of his house from a very early period.  Her mother\n",
      "had died t\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "print(gutenberg.fileids())  # List of available books\n",
    "\n",
    "sample = gutenberg.raw('austen-emma.txt')\n",
    "print(sample[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "374e2480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "gutenberg.fileids()  # List of included texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1032799",
   "metadata": {},
   "source": [
    "# TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37a37e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Sir',\n",
       " '!',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'NLP.',\n",
       " 'Let',\n",
       " \"'s\",\n",
       " 'start',\n",
       " 'learning',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer= TreebankWordTokenizer()\n",
    "\n",
    "text = \"Hello Sir! Welcome to NLP. Let's start learning.\"\n",
    "\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "875625ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', 'mar', 'bushe']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "reg= RegexpStemmer(r\"ing$|s$|e$|able$'\")\n",
    "\n",
    "words= ['cars', 'mars', 'bushes']\n",
    "\n",
    "[reg.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae1842a",
   "metadata": {},
   "source": [
    "# Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f0ac5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'happi', 'fli', 'fair']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer  \n",
    "\n",
    "# Initialize for English\n",
    "stemmer = SnowballStemmer(\"english\")  \n",
    "\n",
    "words = [\"running\", \"happiness\", \"flies\", \"fairly\"]\n",
    "stemmed = [stemmer.stem(word) for word in words]\n",
    "\n",
    "print(stemmed)  \n",
    "# Output: ['run', 'happi', 'fli', 'fair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6cf0492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List available languages\n",
    "SnowballStemmer.languages\n",
    "# ['arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
